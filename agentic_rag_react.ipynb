{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb7c35c6",
      "metadata": {
        "id": "cb7c35c6"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Agentic RAG with ReAct Pattern\n",
        "\n",
        "This exercise demonstrates an advanced RAG system using the ReAct (Reasoning + Acting) pattern.\n",
        "The system can use multiple tools to gather information:\n",
        "- Vector database search with reranking\n",
        "- Web search for current information\n",
        "- Company director information extraction with LinkedIn lookups\n",
        "\n",
        "The objective is to answer complex multi-part questions by intelligently using these tools.\n",
        "\n",
        "# How to Run this Notebook?\n",
        "\n",
        "1. Generate API key for OpenAI (ChatGPT): https://platform.openai.com/settings/organization/api-keys\n",
        "   Make sure to save the API key.\n",
        "\n",
        "2. Generate API key for SerpAPI (for web search): https://serpapi.com/users/sign_up?plan=free\n",
        "\n",
        "3. Generate API key for LangChain to access LangSmith (for tracing): https://smith.langchain.com/settings\n",
        "\n",
        "4. Click on the key icon in the left menu bar of this Notebook\n",
        "\n",
        "5. Click + Add new secret\n",
        "   - Add OpenAI key:\n",
        "     * Under Name: OPEN_AI_KEY\n",
        "     * Under Value: Your OpenAI key\n",
        "   - Add SerpAPI key:\n",
        "     * Under Name: SERP_KEY\n",
        "     * Under Value: Your SerpAPI key\n",
        "- Add LangChain key:\n",
        "     * Under Name: LANG_KEY\n",
        "     * Under Value: Your LangSmith key\n",
        "\n",
        "6. Enable access to the keys for this notebook by toggling the radio buttons.\n",
        "7. Close the Secrets section once done.\n",
        "8. Click Run all under the Runtime menu to execute this notebook.\n",
        "\n",
        "# Basic Setup\n",
        "\n",
        "## Install Frameworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4f679c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df4f679c",
        "outputId": "123470d2-b4de-4514-fa46-ab93ffcce717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.0.7-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_classic\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: langchain_text_splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Collecting langchain_text_splitters\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Collecting openai\n",
            "  Downloading openai-2.8.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting flashrank\n",
            "  Downloading FlashRank-0.2.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.42)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from flashrank) (0.22.1)\n",
            "Collecting onnxruntime (from flashrank)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2024.11.6)\n",
            "Collecting coloredlogs (from onnxruntime->flashrank)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->flashrank) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->flashrank) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime->flashrank) (1.13.3)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->flashrank) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->flashrank) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->flashrank) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->flashrank) (1.2.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->flashrank)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime->flashrank) (1.3.0)\n",
            "Downloading langchain-1.0.7-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.0.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-2.8.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FlashRank-0.2.10-py3-none-any.whl (14 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.4-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=92577e004f3ea14ae10cb63bdfd6e0fb8d3af94be38164709b8c6488b3251696\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/47/f5/89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: requests, ormsgpack, mypy-extensions, marshmallow, humanfriendly, faiss-cpu, typing-inspect, google-search-results, coloredlogs, openai, onnxruntime, langgraph-sdk, dataclasses-json, langchain_core, flashrank, langgraph-checkpoint, langchain_text_splitters, langchain_openai, langgraph-prebuilt, langchain_classic, langgraph, langchain_community, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.79\n",
            "    Uninstalling langchain-core-0.3.79:\n",
            "      Successfully uninstalled langchain-core-0.3.79\n",
            "  Attempting uninstall: langchain_text_splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 dataclasses-json-0.6.7 faiss-cpu-1.12.0 flashrank-0.2.10 google-search-results-2.4.2 humanfriendly-10.0 langchain-1.0.7 langchain_classic-1.0.0 langchain_community-0.4.1 langchain_core-1.0.5 langchain_openai-1.0.3 langchain_text_splitters-1.0.0 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.4 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 onnxruntime-1.23.2 openai-2.8.0 ormsgpack-1.12.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_core langchain_community langchain_openai langchain_classic langchain_text_splitters faiss-cpu openai flashrank google-search-results python-dotenv -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a19a2ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a19a2ca",
        "outputId": "18211fb3-06e7-44ce-ce69-d95514c04e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain version: 1.0.5\n",
            "langchain_core version: 1.0.5\n",
            "langchain_community version: 0.4.1\n",
            "openai version: 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import langchain\n",
        "import langchain_core\n",
        "import langchain_community\n",
        "import openai\n",
        "\n",
        "print(f\"langchain version: {langchain.__version__}\")\n",
        "print(f\"langchain_core version: {langchain_core.__version__}\")\n",
        "print(f\"langchain_community version: {langchain_community.__version__}\")\n",
        "print(f\"openai version: {openai.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0615c0",
      "metadata": {
        "id": "be0615c0"
      },
      "source": [
        "## API Keys Setup\n",
        "\n",
        "Set up API keys for OpenAI, SerpAPI, and LangChain.\n",
        "\n",
        "Important: You need to setup your API keys in Google Colab's Secrets manager or in a .env file for local execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048af1d3",
      "metadata": {
        "id": "048af1d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Check if we're in a Colab environment\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import userdata\n",
        "    # Set environment variables from Colab secrets\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPEN_AI_KEY')\n",
        "    os.environ[\"SERPAPI_API_KEY\"] = userdata.get('SERP_KEY')\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANG_KEY')\n",
        "else:\n",
        "    # For local execution, load from .env file\n",
        "    from dotenv import load_dotenv\n",
        "    # Try to load .env from current directory first, then fall back to specified path\n",
        "    load_dotenv()  # Loads from current directory\n",
        "\n",
        "    # Set environment variables with validation\n",
        "    openai_key = os.getenv('OPENAI_KEY')\n",
        "    serpapi_key = os.getenv('SERPAPI_KEY')\n",
        "    langchain_key = os.getenv('LANGCHAIN_KEY')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64760806",
      "metadata": {
        "id": "64760806"
      },
      "source": [
        "## LangChain Tracing Configuration\n",
        "\n",
        "Configure LangSmith for tracing and debugging the agent's behavior.\n",
        "This helps us see how the agent reasons and which tools it uses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3762153f",
      "metadata": {
        "id": "3762153f"
      },
      "outputs": [],
      "source": [
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"4c_agentic_rag_complex_fall_2025\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "496f13da",
      "metadata": {
        "id": "496f13da"
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "Import all necessary libraries for the agentic RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc6a313",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebc6a313",
        "outputId": "929ed4e2-ca81-4be0-b595-05ae95ba8bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pprint import pprint\n",
        "\n",
        "# LangChain core imports\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.tools import BaseTool\n",
        "\n",
        "# LangChain community imports\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# LangChain OpenAI imports\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "# Flashrank for reranking\n",
        "from flashrank import Ranker, RerankRequest\n",
        "import langchain_community.document_compressors.flashrank_rerank as fr_mod\n",
        "fr_mod.RerankRequest = RerankRequest\n",
        "from langchain_community.document_compressors import FlashrankRerank\n",
        "\n",
        "# For retriever with compression\n",
        "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# Agent imports\n",
        "from langchain_classic.agents import create_react_agent, AgentExecutor\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b39ba6",
      "metadata": {
        "id": "e4b39ba6"
      },
      "source": [
        "# Configuration Dictionary\n",
        "\n",
        "Contains all settings for the agentic RAG system, including:\n",
        "- Document processing parameters\n",
        "- Embedding model settings\n",
        "- Vector store and retrieval settings\n",
        "- Reranker configuration\n",
        "- ReAct agent settings\n",
        "- Tool configurations\n",
        "- Prompt templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbbc2606",
      "metadata": {
        "id": "dbbc2606"
      },
      "outputs": [],
      "source": [
        "defaultConfig = {\n",
        "    # Document processing settings\n",
        "    \"chunkSize\": 500,\n",
        "    \"chunkOverlap\": 50,\n",
        "    \"userAgentHeader\": \"YourCompany-ResearchBot/1.0 (your@email.com)\",\n",
        "\n",
        "    # Embedding model (OpenAI)\n",
        "    \"embeddingModelName\": \"text-embedding-3-small\",\n",
        "\n",
        "    # Vector store settings\n",
        "    \"numRetrievedDocuments\": 12,\n",
        "\n",
        "    # Document formatter settings\n",
        "    \"numSelectedDocuments\": 12,\n",
        "\n",
        "    # Reranker settings (Flashrank)\n",
        "    \"rerankerModel\": \"ms-marco-TinyBERT-L-2-v2\",\n",
        "    \"numRerankedDocuments\": 5,\n",
        "\n",
        "    # Model settings for answer generation\n",
        "    \"ragAnswerModel\": \"gpt-4o\",\n",
        "    \"ragAnswerModelTemperature\": 0.7,\n",
        "\n",
        "    # URLs to process - Multiple 10-K filings\n",
        "    \"companyFilingUrls\": [\n",
        "        (\"Tesla\", \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\"),\n",
        "        (\"General Motors\", \"https://www.sec.gov/Archives/edgar/data/1467858/000146785824000031/gm-20231231.htm\")\n",
        "    ],\n",
        "\n",
        "    # RAG prompt template\n",
        "    \"ragPromptTemplate\": \"\"\"\n",
        "    Give an answer for the Question using only the given Context. Use information relevant to the query from the entire context.\n",
        "    Provide a detailed answer with thorough explanations, avoiding summaries.\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Context: {context}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\",\n",
        "\n",
        "    # ReAct Agent settings\n",
        "    \"reactModelName\": \"gpt-4o\",\n",
        "    \"reactModelTemperature\": 0,\n",
        "\n",
        "    \"reactPromptTemplate\": \"\"\"Your task is to gather relevant information to build context for the question. Focus on collecting details related to the question.\n",
        "    Gather as much context as possible before formulating your answer.\n",
        "\n",
        "    You have access to the following tools:\n",
        "\n",
        "    {tools}\n",
        "\n",
        "    Use the following format:\n",
        "\n",
        "    Question: the input question you must answer\n",
        "\n",
        "    Thought: you should always think about what to do\n",
        "\n",
        "    Action: the action to take, should be one of [{tool_names}]\n",
        "\n",
        "    Action Input: the input to the action\n",
        "\n",
        "    Observation: the result of the action\n",
        "\n",
        "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "\n",
        "    Thought: I now know the final answer\n",
        "\n",
        "    Final Answer: the final answer to the question.\n",
        "\n",
        "    Follow these steps:\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Question: {input}\n",
        "\n",
        "    Thought:{agent_scratchpad}\n",
        "    \"\"\",\n",
        "\n",
        "    \"reactVerbosity\": True,\n",
        "\n",
        "    # Name Extraction settings\n",
        "    \"nameExtractionModel\": \"gpt-4o-mini\",\n",
        "    \"nameExtractionModelTemperature\": 0.4,\n",
        "    \"nameExtractionPrompt\": \"\"\"\n",
        "    Extract and list the names of all individuals with the title 'Director' from the following text, excluding any additional information such as dates or signatures.\n",
        "    Present the names as a simple, comma-separated list.\n",
        "\n",
        "    {text}\n",
        "    \"\"\",\n",
        "\n",
        "    # Tool settings\n",
        "    \"useDirectorTool\": True,\n",
        "    \"directorToolName\": \"Company Directors Information\",\n",
        "    \"directorToolDescription\": \"Retrieve the names of company directors for a chosen company. Optionally, their LinkedIn handles can also be included. Use the format: company_name, true/false.\",\n",
        "\n",
        "    \"useWebTool\": True,\n",
        "    \"webToolName\": \"WebSearch\",\n",
        "    \"webToolDescription\": \"Performs a web search on the query.\",\n",
        "    \"numWebToolResults\": 3,\n",
        "\n",
        "    \"useRetrieverTool\": True,\n",
        "    \"retrieverToolName\": \"Vector Reranker Search\",\n",
        "    \"retrieverToolDescription\": \"Retrieves information from an embedding based vector DB containing financial data and company information. Structure query as a sentence.\",\n",
        "    \"numRetrieverToolResults\": 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e94cb24",
      "metadata": {
        "id": "8e94cb24"
      },
      "outputs": [],
      "source": [
        "config = defaultConfig.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24ebb54",
      "metadata": {
        "id": "e24ebb54"
      },
      "source": [
        "# Document Processing Functions\n",
        "\n",
        "Functions to load and process company 10-K filings from SEC website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f1b739",
      "metadata": {
        "id": "10f1b739"
      },
      "outputs": [],
      "source": [
        "def load_and_process_filings(urls: List[Tuple[str, str]], config: Dict[str, Any]) -> Tuple[List[Document], Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Load and process company filings from URLs.\n",
        "\n",
        "    Args:\n",
        "        urls: List of tuples (company_name, url)\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (processed_chunks, director_sections)\n",
        "    \"\"\"\n",
        "    processed_chunks = []\n",
        "    director_sections = {}\n",
        "\n",
        "    # Create text splitter\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config[\"chunkSize\"],\n",
        "        chunk_overlap=config[\"chunkOverlap\"]\n",
        "    )\n",
        "\n",
        "    for company, url in urls:\n",
        "        try:\n",
        "            print(f\"Loading {company} filing from {url}\")\n",
        "\n",
        "            # Load document using WebBaseLoader\n",
        "            loader = WebBaseLoader(\n",
        "                url,\n",
        "                header_template={'User-Agent': config[\"userAgentHeader\"]}\n",
        "            )\n",
        "            docs = loader.load()\n",
        "\n",
        "            # Store last 1000 characters for director extraction\n",
        "            if docs and len(docs) > 0:\n",
        "                director_sections[company] = docs[0].page_content[-1000:]\n",
        "\n",
        "            # Split documents into chunks\n",
        "            chunks = splitter.transform_documents(docs)\n",
        "\n",
        "            # Add company metadata to each chunk\n",
        "            for chunk in chunks:\n",
        "                chunk.metadata[\"company\"] = company\n",
        "\n",
        "            processed_chunks.extend(chunks)\n",
        "            print(f\"Processed {len(chunks)} chunks from {company}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {company} from {url}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Total processed chunks: {len(processed_chunks)}\")\n",
        "    return processed_chunks, director_sections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f18f88",
      "metadata": {
        "id": "40f18f88"
      },
      "source": [
        "# Vector Store and Retriever Functions\n",
        "\n",
        "Functions to create the vector store and retriever with reranking capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1bca2c",
      "metadata": {
        "id": "1c1bca2c"
      },
      "outputs": [],
      "source": [
        "def create_vector_store(chunks: List[Document], config: Dict[str, Any]) -> FAISS:\n",
        "    \"\"\"\n",
        "    Create a FAISS vector store with OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of document chunks\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store\n",
        "    \"\"\"\n",
        "    print(\"Creating vector store with OpenAI embeddings...\")\n",
        "\n",
        "    # Create embedding function\n",
        "    embedding_function = OpenAIEmbeddings(\n",
        "        model=config[\"embeddingModelName\"]\n",
        "    )\n",
        "\n",
        "    # Create FAISS vector store\n",
        "    vectorstore = FAISS.from_documents(chunks, embedding_function)\n",
        "\n",
        "    print(f\"Vector store created successfully\")\n",
        "    print(f\"Number of vectors: {vectorstore.index.ntotal}\")\n",
        "    print(f\"Vector dimension: {vectorstore.index.d}\")\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_retriever_with_reranking(vectorstore: FAISS, config: Dict[str, Any]):\n",
        "    \"\"\"\n",
        "    Create a retriever with Flashrank reranking.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: FAISS vector store\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        ContextualCompressionRetriever with reranking\n",
        "    \"\"\"\n",
        "    print(\"Creating retriever with Flashrank reranking...\")\n",
        "\n",
        "    # Create base retriever\n",
        "    base_retriever = vectorstore.as_retriever(\n",
        "        search_kwargs={\"k\": config[\"numRetrievedDocuments\"]}\n",
        "    )\n",
        "\n",
        "    # Create Flashrank reranker\n",
        "    model_name = config[\"rerankerModel\"]\n",
        "    top_n = config[\"numRerankedDocuments\"]\n",
        "\n",
        "    ranker_client = Ranker(model_name=model_name)\n",
        "    reranker = FlashrankRerank(client=ranker_client, model=model_name, top_n=top_n)\n",
        "\n",
        "    # Create compression retriever with reranker\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_retriever=base_retriever,\n",
        "        base_compressor=reranker\n",
        "    )\n",
        "\n",
        "    print(\"Retriever with reranking created successfully\")\n",
        "    return compression_retriever"
      ],
      "metadata": {
        "id": "ktzPLXeJ5Ehs"
      },
      "id": "ktzPLXeJ5Ehs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b03e8f3d",
      "metadata": {
        "id": "b03e8f3d"
      },
      "source": [
        "# Custom Tool Classes\n",
        "\n",
        "Three custom tools for the ReAct agent:\n",
        "1. CompanyDirectorsTool - Extracts director names and finds LinkedIn profiles\n",
        "2. WebSearchTool - Performs web searches using SerpAPI\n",
        "3. VectorRerankerSearchTool - Searches the vector database with reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da8c75a5",
      "metadata": {
        "id": "da8c75a5"
      },
      "outputs": [],
      "source": [
        "# Cache for LinkedIn lookups to avoid repeated API calls\n",
        "linkedin_cache = {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa50d7e1",
      "metadata": {
        "id": "fa50d7e1"
      },
      "source": [
        "Tool to retrieve company director names and LinkedIn profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6352ca",
      "metadata": {
        "id": "bb6352ca"
      },
      "outputs": [],
      "source": [
        "class CompanyDirectorsTool(BaseTool):\n",
        "    \"\"\"Tool to retrieve company director names and LinkedIn profiles.\"\"\"\n",
        "\n",
        "    name: str = \"Company Directors Information\"\n",
        "    description: str = \"Retrieve the names of company directors for a chosen company. Optionally, their LinkedIn handles can also be included. Use the format: company_name, true/false.\"\n",
        "\n",
        "    # Custom attributes\n",
        "    director_sections: Dict[str, str] = {}\n",
        "    config: Dict[str, Any] = {}\n",
        "\n",
        "    def __init__(self, director_sections: Dict[str, str], config: Dict[str, Any]):\n",
        "        \"\"\"Initialize the tool with director sections and config.\"\"\"\n",
        "        # Update description with available companies\n",
        "        available_companies = list(director_sections.keys())\n",
        "        updated_description = f\"{config['directorToolDescription']} Available companies: {', '.join(available_companies)}\"\n",
        "\n",
        "        super().__init__(\n",
        "            director_sections=director_sections,\n",
        "            config=config,\n",
        "            description=updated_description\n",
        "        )\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Execute the tool to get director information.\"\"\"\n",
        "        try:\n",
        "            # Parse input\n",
        "            parts = query.split(',')\n",
        "            company_name = parts[0].strip()\n",
        "            include_linkedin = parts[1].strip().lower() == 'true' if len(parts) > 1 else True\n",
        "\n",
        "            # Get director section\n",
        "            company_snippet = self.director_sections.get(company_name)\n",
        "            if not company_snippet:\n",
        "                return f\"No director information found for {company_name}\"\n",
        "\n",
        "            # Extract director names using LLM\n",
        "            director_names = self._extract_director_names(company_snippet)\n",
        "\n",
        "            if not director_names:\n",
        "                return f\"Could not extract director names for {company_name}\"\n",
        "\n",
        "            # Get LinkedIn handles if requested\n",
        "            if include_linkedin:\n",
        "                director_handles = []\n",
        "                for name in director_names:\n",
        "                    linkedin_handle = self._get_linkedin_handle(name, company_name)\n",
        "                    director_handles.append(f\"{name} (LinkedIn: {linkedin_handle})\")\n",
        "\n",
        "                return f\"Directors of {company_name}: {'; '.join(director_handles)}\"\n",
        "            else:\n",
        "                return f\"Directors of {company_name}: {', '.join(director_names)}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving director information: {str(e)}\"\n",
        "\n",
        "    def _extract_director_names(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract director names from text using LLM.\"\"\"\n",
        "        try:\n",
        "            llm = ChatOpenAI(\n",
        "                model=self.config[\"nameExtractionModel\"],\n",
        "                temperature=self.config[\"nameExtractionModelTemperature\"]\n",
        "            )\n",
        "            parser = CommaSeparatedListOutputParser()\n",
        "            prompt = PromptTemplate.from_template(self.config[\"nameExtractionPrompt\"])\n",
        "\n",
        "            chain = prompt | llm | parser\n",
        "            names = chain.invoke({\"text\": text})\n",
        "            return names\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting names: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _get_linkedin_handle(self, name: str, company: str) -> str:\n",
        "        \"\"\"Get LinkedIn handle for a director.\"\"\"\n",
        "        cache_key = f\"{name}_{company}\"\n",
        "\n",
        "        # Check cache first\n",
        "        if cache_key in linkedin_cache:\n",
        "            return linkedin_cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            # Use SerpAPI to search LinkedIn\n",
        "            search = SerpAPIWrapper()\n",
        "            results = search.results(f'\"{name}\" {company} site:linkedin.com/in/')\n",
        "\n",
        "            # Extract link from results\n",
        "            link = results.get(\"organic_results\", [{}])[0].get(\"link\", \"Profile not found\")\n",
        "\n",
        "            # Cache the result\n",
        "            linkedin_cache[cache_key] = link\n",
        "            return link\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error finding LinkedIn profile: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WebSearchTool(BaseTool):\n",
        "    \"\"\"Tool to perform web searches using SerpAPI.\"\"\"\n",
        "\n",
        "    name: str = \"WebSearch\"\n",
        "    description: str = \"Performs a web search on the query.\"\n",
        "\n",
        "    # Custom attributes\n",
        "    config: Dict[str, Any] = {}\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        \"\"\"Initialize the tool with configuration.\"\"\"\n",
        "        super().__init__(config=config)\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Execute the web search.\"\"\"\n",
        "        try:\n",
        "            search = SerpAPIWrapper()\n",
        "            results = search.results(query)\n",
        "            return self._format_results(results)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error performing web search: {str(e)}\"\n",
        "\n",
        "    def _format_results(self, results: Dict) -> str:\n",
        "        \"\"\"Format search results into readable text.\"\"\"\n",
        "        formatted = []\n",
        "        num_results = self.config[\"numWebToolResults\"]\n",
        "\n",
        "        for result in results.get(\"organic_results\", [])[:num_results]:\n",
        "            formatted.append(\n",
        "                f\"Title: {result.get('title', 'N/A')}\\n\"\n",
        "                f\"Snippet: {result.get('snippet', 'N/A')}\\n\"\n",
        "                f\"Link: {result.get('link', 'N/A')}\"\n",
        "            )\n",
        "\n",
        "        return \"\\n\\n\".join(formatted)"
      ],
      "metadata": {
        "id": "BrKa6O_1vSik"
      },
      "id": "BrKa6O_1vSik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorRerankerSearchTool(BaseTool):\n",
        "    \"\"\"Tool to search the vector database with reranking.\"\"\"\n",
        "\n",
        "    name: str = \"Vector Reranker Search\"\n",
        "    description: str = \"Retrieves information from an embedding based vector DB containing financial data and company information. Structure query as a sentence.\"\n",
        "\n",
        "    # Custom attributes\n",
        "    retriever: Any = None\n",
        "    config: Dict[str, Any] = {}\n",
        "\n",
        "    def __init__(self, retriever: Any, config: Dict[str, Any]):\n",
        "        \"\"\"Initialize the tool with retriever and config.\"\"\"\n",
        "        super().__init__(retriever=retriever, config=config)\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"Execute the vector search with reranking.\"\"\"\n",
        "        try:\n",
        "            # Retrieve documents\n",
        "            docs = self.retriever.invoke(query)\n",
        "\n",
        "            # Format and return top results\n",
        "            num_results = self.config[\"numRetrieverToolResults\"]\n",
        "            formatted_docs = []\n",
        "\n",
        "            for doc in docs[:num_results]:\n",
        "                company = doc.metadata.get('company', '')\n",
        "                content = doc.page_content\n",
        "                formatted_docs.append(f\"{company}\\n{content}\")\n",
        "\n",
        "            return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error retrieving documents: {str(e)}\""
      ],
      "metadata": {
        "id": "PAIF2yw5vTVN"
      },
      "id": "PAIF2yw5vTVN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b58927ac",
      "metadata": {
        "id": "b58927ac"
      },
      "source": [
        "# Agent Setup Functions\n",
        "\n",
        "Functions to create and configure the ReAct agent with all tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "373e7c78",
      "metadata": {
        "id": "373e7c78"
      },
      "outputs": [],
      "source": [
        "def create_tools(config: Dict[str, Any], director_sections: Dict[str, str], retriever) -> List[BaseTool]:\n",
        "    \"\"\"\n",
        "    Create all tools for the ReAct agent.\n",
        "\n",
        "    Args:\n",
        "        config: Configuration dictionary\n",
        "        director_sections: Dictionary mapping company names to director text sections\n",
        "        retriever: Retriever with reranking\n",
        "\n",
        "    Returns:\n",
        "        List of tools\n",
        "    \"\"\"\n",
        "    tools = []\n",
        "\n",
        "    # Add Company Directors Tool\n",
        "    if config.get(\"useDirectorTool\", False):\n",
        "        director_tool = CompanyDirectorsTool(director_sections, config)\n",
        "        tools.append(director_tool)\n",
        "        print(f\"Added tool: {director_tool.name}\")\n",
        "\n",
        "    # Add Web Search Tool\n",
        "    if config.get(\"useWebTool\", False):\n",
        "        web_tool = WebSearchTool(config)\n",
        "        tools.append(web_tool)\n",
        "        print(f\"Added tool: {web_tool.name}\")\n",
        "\n",
        "    # Add Vector Reranker Search Tool\n",
        "    if config.get(\"useRetrieverTool\", False):\n",
        "        retriever_tool = VectorRerankerSearchTool(retriever, config)\n",
        "        tools.append(retriever_tool)\n",
        "        print(f\"Added tool: {retriever_tool.name}\")\n",
        "\n",
        "    print(f\"Total tools created: {len(tools)}\")\n",
        "    return tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_react_agent_executor(tools: List[BaseTool], config: Dict[str, Any]) -> AgentExecutor:\n",
        "    \"\"\"\n",
        "    Create a ReAct agent executor with the specified tools.\n",
        "\n",
        "    Args:\n",
        "        tools: List of tools for the agent\n",
        "        config: Configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        AgentExecutor\n",
        "    \"\"\"\n",
        "    print(\"Creating ReAct agent...\")\n",
        "\n",
        "    # Create LLM for the agent\n",
        "    llm = ChatOpenAI(\n",
        "        model=config[\"reactModelName\"],\n",
        "        temperature=config[\"reactModelTemperature\"]\n",
        "    )\n",
        "\n",
        "    # Create prompt template\n",
        "    prompt = PromptTemplate.from_template(config[\"reactPromptTemplate\"])\n",
        "\n",
        "    # Create ReAct agent\n",
        "    agent = create_react_agent(llm, tools, prompt)\n",
        "\n",
        "    # Create agent executor\n",
        "    agent_executor = AgentExecutor(\n",
        "        agent=agent,\n",
        "        tools=tools,\n",
        "        verbose=config.get(\"reactVerbosity\", True),\n",
        "        handle_parsing_errors=True\n",
        "    )\n",
        "\n",
        "    print(\"ReAct agent created successfully\")\n",
        "    return agent_executor"
      ],
      "metadata": {
        "id": "_E-ZmzFQvim7"
      },
      "id": "_E-ZmzFQvim7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1792a79d",
      "metadata": {
        "id": "1792a79d"
      },
      "source": [
        "# Main Execution Flow\n",
        "\n",
        "Load documents, create vector store, set up tools, and run the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8053bbc",
      "metadata": {
        "id": "f8053bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e350c6d0-7354-4019-cf48-4a343e6126d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: Loading and Processing Documents\n",
            "============================================================\n",
            "Loading Tesla filing from https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\n",
            "Processed 942 chunks from Tesla\n",
            "Loading General Motors filing from https://www.sec.gov/Archives/edgar/data/1467858/000146785824000031/gm-20231231.htm\n",
            "Processed 1085 chunks from General Motors\n",
            "Total processed chunks: 2027\n",
            "\n",
            "============================================================\n",
            "STEP 2: Creating Vector Store\n",
            "============================================================\n",
            "Creating vector store with OpenAI embeddings...\n",
            "Vector store created successfully\n",
            "Number of vectors: 2027\n",
            "Vector dimension: 1536\n",
            "\n",
            "============================================================\n",
            "STEP 3: Creating Retriever with Reranking\n",
            "============================================================\n",
            "Creating retriever with Flashrank reranking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ms-marco-TinyBERT-L-2-v2.zip: 100%|██████████| 3.26M/3.26M [00:00<00:00, 13.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever with reranking created successfully\n",
            "\n",
            "============================================================\n",
            "STEP 4: Creating Tools for Agent\n",
            "============================================================\n",
            "Added tool: Company Directors Information\n",
            "Added tool: WebSearch\n",
            "Added tool: Vector Reranker Search\n",
            "Total tools created: 3\n",
            "\n",
            "============================================================\n",
            "STEP 5: Creating ReAct Agent\n",
            "============================================================\n",
            "Creating ReAct agent...\n",
            "ReAct agent created successfully\n",
            "\n",
            "============================================================\n",
            "STEP 6: Running Agent with Complex Query\n",
            "============================================================\n",
            "\n",
            "Question: Who are the directors of Tesla. What are their linkedin handles? What are the financial goals of tesla this year. What is the next auto show that Tesla will participate in.\n",
            "\n",
            "Agent is working...\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer the question, I need to gather information on the directors of Tesla, their LinkedIn handles, Tesla's financial goals for this year, and the next auto show Tesla will participate in.\n",
            "\n",
            "Action: Company Directors Information\n",
            "Action Input: Tesla, true\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mDirectors of Tesla: Elon Musk (LinkedIn: https://www.linkedin.com/in/elon-musk-tesla-company-b7b00879); Robyn Denholm (LinkedIn: https://au.linkedin.com/in/robyn-denholm-a807795); Ira Ehrenpreis (LinkedIn: https://www.linkedin.com/in/iraehrenpreis); Joseph Gebbia (LinkedIn: Profile not found); James Murdoch (LinkedIn: https://www.linkedin.com/in/jamesrmurdoch); Kimbal Musk (LinkedIn: https://www.linkedin.com/in/kimbal); JB Straubel (LinkedIn: https://www.linkedin.com/in/jb-straubel-b694981); Kathleen Wilson-Thompson (LinkedIn: https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered the information on the directors of Tesla and their LinkedIn handles. Now, I need to find out Tesla's financial goals for this year and the next auto show they will participate in.\n",
            "\n",
            "Action: Vector Reranker Search\n",
            "Action Input: What are Tesla's financial goals for this year?\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mTesla\n",
            "consolidated balance sheets of Tesla, Inc. and its subsidiaries (the “Company”) as of December 31, 2023 and 2022, and the related consolidated statements of operations, of comprehensive income, of redeemable noncontrolling interests and equity and of cash flows for each of the three years in the period ended December 31, 2023, including the related notes (collectively referred to as the “consolidated financial statements”). We also have audited the Company's internal control over financial\n",
            "\n",
            "Tesla\n",
            "which oversees our ESG impacts, initiatives and priorities.Human Capital ResourcesA competitive edge for Tesla is its ability to attract and retain high quality employees. During the past year, Tesla made substantial investments in its workforce, further strengthening its standing as one of the most desirable and innovative companies to work for. As of December 31, 2023, our employee headcount worldwide was 140,473.We have created an environment that fosters growth opportunities, and as of this\n",
            "\n",
            "Tesla\n",
            "long-term liabilities— 3 Total liabilities$3,777 $2,215 Note 17 – Related Party TransactionsIn relation to our CEO’s exercise of stock options and sale of common stock from the 2012 CEO Performance Award, Tesla withheld the appropriate amount of taxes. However, given the significant amounts involved, our CEO entered into an indemnification agreement with us in November 2021 for additional taxes owed, if any. Tesla periodically does business with certain entities with which its CEO and directors\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered some financial information about Tesla, but it doesn't specifically address their financial goals for this year. I need to perform another search to find more targeted information about Tesla's financial goals for 2023.\n",
            "\n",
            "Action: WebSearch\n",
            "Action Input: Tesla financial goals 2023\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mTitle: Tesla Full Year 2023 Analysis — A Review of Actual ...\n",
            "Snippet: Tesla earned $13.2 billion in Operating Cash Flow and $4.3 billion in Free Cash Flow in 2023, spending $8.9 billion on Capital Expenditure. The ...\n",
            "Link: https://medium.com/@nambos3rd/tesla-full-year-2023-analysis-a-review-of-actual-performance-my-financial-forecast-579771d7d452\n",
            "\n",
            "Title: Tesla's 2023 Impact Report\n",
            "Snippet: In addition, we aim to transition our operational electricity load to 100% renewables well before we achieve our net-zero emissions goal and to continue.\n",
            "Link: https://www.tesla.com/ns_videos/2023-tesla-impact-report-highlights.pdf\n",
            "\n",
            "Title: tsla-20231231-gen.pdf\n",
            "Snippet: We also have audited the Company's internal control over financial reporting as of December 31, 2023, based on criteria established in.\n",
            "Link: https://ir.tesla.com/_flysystem/s3/sec/000162828024002390/tsla-20231231-gen.pdf\u001b[0m\u001b[32;1m\u001b[1;3mI have found some information about Tesla's financial performance in 2023, but I still need to find specific financial goals for the year. Additionally, I need to find information about the next auto show Tesla will participate in.\n",
            "\n",
            "Action: WebSearch\n",
            "Action Input: Tesla next auto show participation 2023\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mTitle: Tesla to attend 2023 Detroit Auto Show\n",
            "Snippet: The 2023 Detroit Auto Show would run from September 13 to 24 at the Huntington Place in Detroit. Tesla, together with BMW, Cadillac, Chevrolet, ...\n",
            "Link: https://www.teslarati.com/tesla-to-attend-2023-detroit-auto-show/\n",
            "\n",
            "Title: It's On: Detroit Auto Show Announces BMW, Cadillac ...\n",
            "Snippet: It's On: Detroit Auto Show Announces BMW, Cadillac, Chevrolet, GMC, Ford, Tesla and Volkswagen to Participate in All-New Indoor EV Track. August 31, 2023. Share ...\n",
            "Link: https://detroitautoshow.com/news/its-on-detroit-auto-show-announces-bmw-cadillac-chevrolet-gmc-ford-tesla-and-volkswagen-to-participate-in-all-new-indoor-ev-track/\n",
            "\n",
            "Title: Tesla to Join 2023 Detroit Auto Show\n",
            "Snippet: Tesla will join the 2023 Detroit Auto Show that will take place September 13-24. The company will showcase its vehicles at the Powering ...\n",
            "Link: https://driveteslacanada.ca/news/tesla-to-join-2023-detroit-auto-show/\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered the necessary information to answer the question.\n",
            "\n",
            "Final Answer: The directors of Tesla are Elon Musk, Robyn Denholm, Ira Ehrenpreis, Joseph Gebbia, James Murdoch, Kimbal Musk, JB Straubel, and Kathleen Wilson-Thompson. Their LinkedIn handles are as follows:\n",
            "- Elon Musk: [LinkedIn](https://www.linkedin.com/in/elon-musk-tesla-company-b7b00879)\n",
            "- Robyn Denholm: [LinkedIn](https://au.linkedin.com/in/robyn-denholm-a807795)\n",
            "- Ira Ehrenpreis: [LinkedIn](https://www.linkedin.com/in/iraehrenpreis)\n",
            "- Joseph Gebbia: LinkedIn profile not found\n",
            "- James Murdoch: [LinkedIn](https://www.linkedin.com/in/jamesrmurdoch)\n",
            "- Kimbal Musk: [LinkedIn](https://www.linkedin.com/in/kimbal)\n",
            "- JB Straubel: [LinkedIn](https://www.linkedin.com/in/jb-straubel-b694981)\n",
            "- Kathleen Wilson-Thompson: [LinkedIn](https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)\n",
            "\n",
            "Tesla's financial goals for 2023 include transitioning their operational electricity load to 100% renewables well before achieving their net-zero emissions goal.\n",
            "\n",
            "The next auto show Tesla will participate in is the 2023 Detroit Auto Show, which runs from September 13 to 24 at the Huntington Place in Detroit.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "============================================================\n",
            "FINAL ANSWER\n",
            "============================================================\n",
            "The directors of Tesla are Elon Musk, Robyn Denholm, Ira Ehrenpreis, Joseph Gebbia, James Murdoch, Kimbal Musk, JB Straubel, and Kathleen Wilson-Thompson. Their LinkedIn handles are as follows:\n",
            "- Elon Musk: [LinkedIn](https://www.linkedin.com/in/elon-musk-tesla-company-b7b00879)\n",
            "- Robyn Denholm: [LinkedIn](https://au.linkedin.com/in/robyn-denholm-a807795)\n",
            "- Ira Ehrenpreis: [LinkedIn](https://www.linkedin.com/in/iraehrenpreis)\n",
            "- Joseph Gebbia: LinkedIn profile not found\n",
            "- James Murdoch: [LinkedIn](https://www.linkedin.com/in/jamesrmurdoch)\n",
            "- Kimbal Musk: [LinkedIn](https://www.linkedin.com/in/kimbal)\n",
            "- JB Straubel: [LinkedIn](https://www.linkedin.com/in/jb-straubel-b694981)\n",
            "- Kathleen Wilson-Thompson: [LinkedIn](https://www.linkedin.com/in/kathleen-wilson-thompson-275654201)\n",
            "\n",
            "Tesla's financial goals for 2023 include transitioning their operational electricity load to 100% renewables well before achieving their net-zero emissions goal.\n",
            "\n",
            "The next auto show Tesla will participate in is the 2023 Detroit Auto Show, which runs from September 13 to 24 at the Huntington Place in Detroit.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and process documents\n",
        "print(\"=\" * 60)\n",
        "print(\"STEP 1: Loading and Processing Documents\")\n",
        "print(\"=\" * 60)\n",
        "processed_chunks, director_sections = load_and_process_filings(\n",
        "    config[\"companyFilingUrls\"],\n",
        "    config\n",
        ")\n",
        "\n",
        "# Step 2: Create vector store\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: Creating Vector Store\")\n",
        "print(\"=\" * 60)\n",
        "vectorstore = create_vector_store(processed_chunks, config)\n",
        "\n",
        "# Step 3: Create retriever with reranking\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: Creating Retriever with Reranking\")\n",
        "print(\"=\" * 60)\n",
        "retriever = create_retriever_with_reranking(vectorstore, config)\n",
        "\n",
        "# Step 4: Create tools\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: Creating Tools for Agent\")\n",
        "print(\"=\" * 60)\n",
        "tools = create_tools(config, director_sections, retriever)\n",
        "\n",
        "# Step 5: Create ReAct agent\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: Creating ReAct Agent\")\n",
        "print(\"=\" * 60)\n",
        "agent_executor = create_react_agent_executor(tools, config)\n",
        "\n",
        "# Step 6: Ask a complex question\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 6: Running Agent with Complex Query\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "question = \"Who are the directors of Tesla. What are their linkedin handles? What are the financial goals of tesla this year. What is the next auto show that Tesla will participate in.\"\n",
        "\n",
        "print(f\"\\nQuestion: {question}\\n\")\n",
        "print(\"Agent is working...\\n\")\n",
        "\n",
        "response = agent_executor.invoke({\"input\": question})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL ANSWER\")\n",
        "print(\"=\" * 60)\n",
        "print(response['output'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}